{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e751e57-13a8-48e3-adc3-d2f6b501c7a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6a14020-674d-4200-9f8c-b2d6dcecc559",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78170bb3-9f86-4923-b8b4-b38cbda545f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.special import expit\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "562d8757-5c9c-416d-aac9-2252e7a50dce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def __preprocess():\n",
    "    filepath = 'data/'\n",
    "    # Initialize a Counter object to simplify word counting\n",
    "    word_count = Counter()\n",
    "\n",
    "    with open(filepath + 'training_label.json', 'r') as f:\n",
    "        file = json.load(f)\n",
    "\n",
    "    # Process each sentence in the data\n",
    "    for d in file:\n",
    "        for s in d['caption']:\n",
    "            # Simplify punctuation removal and split in one step\n",
    "            word_sentence = re.sub('[.!,;?]', ' ', s).lower().split()\n",
    "            # Update the word counts for all words in the sentence\n",
    "            word_count.update(word_sentence)\n",
    "\n",
    "    # Filter out words with occurrences fewer than 5\n",
    "    word_dict = {word: count for word, count in word_count.items() if count > 4}\n",
    "\n",
    "    # Initial tokens for special purposes\n",
    "    useful_tokens = [('<PAD>', 0), ('<SOS>', 1), ('<EOS>', 2), ('<UNK>', 3)]\n",
    "    \n",
    "    # Create index-to-word and word-to-index dictionaries, incorporating special tokens\n",
    "    i2w = {i + len(useful_tokens): w for i, w in enumerate(word_dict)}\n",
    "    w2i = {w: i + len(useful_tokens) for i, w in enumerate(word_dict)}\n",
    "    for token, index in useful_tokens:\n",
    "        i2w[index] = token\n",
    "        w2i[token] = index\n",
    "\n",
    "    return i2w, w2i, word_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4158db7-9fb0-46a9-baf8-cf516b2c1e32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def s_split(sentence, word_dict, w2i):\n",
    "    # Normalize and split the sentence into words\n",
    "    words = re.sub(r'[.!,;?]', ' ', sentence).split()\n",
    "\n",
    "    # Convert words to their corresponding indices in w2i, defaulting to <UNK> index if not found\n",
    "    indexed_sentence = [w2i.get(word, 3) for word in words]\n",
    "\n",
    "    # Prepend <SOS> and append <EOS> tokens\n",
    "    indexed_sentence = [1] + indexed_sentence + [2]\n",
    "\n",
    "    return indexed_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38d86b10-24d9-48d2-8a69-963b094dbf4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def annotate(label_file, word_dict, w2i):\n",
    "    # Define the full path to the label file\n",
    "    label_json_path = f'data/{label_file}'\n",
    "    annotated_captions = []\n",
    "\n",
    "    # Open and load the label file\n",
    "    with open(label_json_path, 'r') as file:\n",
    "        labels = json.load(file)\n",
    "\n",
    "    # Iterate over each data entry in the loaded labels\n",
    "    for data_entry in labels:\n",
    "        # Process each caption using the s_split function\n",
    "        for caption in data_entry['caption']:\n",
    "            indexed_caption = s_split(caption, word_dict, w2i)\n",
    "            annotated_captions.append((data_entry['id'], indexed_caption))\n",
    "\n",
    "    return annotated_captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ada08f2f-dc64-4386-8c48-523061f3c928",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def avi(label_file):\n",
    "    avi_data = {}\n",
    "    i = 1\n",
    "    for file in os.listdir(label_file):\n",
    "        print(\"Loading file no:-  \" + str(i))\n",
    "        i += 1\n",
    "        value = np.load(os.path.join(label_file, file))\n",
    "        if value.size < 80 * 4096:  # If the array is smaller, pad it\n",
    "            padded_value = np.pad(value, (0, 80 * 4096 - value.size), 'constant')\n",
    "            avi_data[file.split('.npy')[0]] = padded_value.reshape(80, 4096)\n",
    "        elif value.size > 80 * 4096:  # If the array is larger, truncate it\n",
    "            truncated_value = value[:80 * 4096]\n",
    "            avi_data[file.split('.npy')[0]] = truncated_value.reshape(80, 4096)\n",
    "        else:\n",
    "            avi_data[file.split('.npy')[0]] = value.reshape(80, 4096)\n",
    "    return avi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afaf96dd-b1cc-4344-abdb-5a797d47b9fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def minibatch(data):\n",
    "    # Sort the data by the length of captions in descending order\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    # Unpack the data into separate lists of avi_data and captions\n",
    "    avi_data, captions = zip(*data)\n",
    "    \n",
    "    # Stack the avi_data into a tensor\n",
    "    avi_data = torch.stack(avi_data, 0)\n",
    "    \n",
    "    # Get the lengths of each caption\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    \n",
    "    # Initialize a zero tensor for targets with dimensions [batch_size, max_caption_length]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    \n",
    "    # Fill in the targets tensor with caption indices\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = torch.tensor(cap[:end], dtype=torch.long)\n",
    "    \n",
    "    # Return the avi_data, targets tensor, and lengths of each caption\n",
    "    return avi_data, targets, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f90ee9c2-4de1-4101-a26a-7b9fa4116a43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainingData(Dataset):\n",
    "    def __init__(self, label_file, files_dir, word_dict, w2i):\n",
    "        self.label_file = label_file\n",
    "        self.files_dir = files_dir\n",
    "        self.word_dict = word_dict\n",
    "        self.avi = avi(label_file)\n",
    "        self.w2i = w2i\n",
    "        self.data_pair = annotate(files_dir, word_dict, w2i)\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Returns the number of items in the dataset\n",
    "        return len(self.data_pair)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #Check\n",
    "        assert idx < self.__len__(), \"Index out of range\"\n",
    "        \n",
    "        avi_file_name, sentence = self.data_pair[idx]\n",
    "        # Get the video data as a tensor and add some noise\n",
    "        video_data = torch.Tensor(self.avi[avi_file_name])\n",
    "        video_data += torch.Tensor(video_data.size()).random_(0, 2000) / 10000.\n",
    "        # Convert sentence to tensor\n",
    "        caption_tensor = torch.Tensor(sentence).long()\n",
    "        \n",
    "        return video_data, caption_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21e21f82-c697-41d5-a386-99fcbca275a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestingData(Dataset):\n",
    "    def __init__(self, test_data_path):\n",
    "        self.avi = []\n",
    "        files = os.listdir(test_data_path)\n",
    "        for file in files:\n",
    "            key = file.split('.npy')[0]\n",
    "            value = np.load(os.path.join(test_data_path, file))\n",
    "            self.avi.append([key, value])\n",
    "            \n",
    "    def __len__(self):\n",
    "        # Returns the number of items in the dataset\n",
    "        return len(self.avi)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Returns the idx-th item of the dataset\n",
    "        assert idx < len(self), \"Index out of range\"\n",
    "        return self.avi[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff47b11f-fb0f-4e8a-a1ae-311c3026489e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_layer1 = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.attention_layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.attention_layer3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.attention_layer4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.compute_weight = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden_state, encoder_outputs):\n",
    "        batch_size, seq_len, hidden_dim = encoder_outputs.size()\n",
    "        hidden_state_expanded = hidden_state.view(batch_size, 1, hidden_dim).repeat(1, seq_len, 1)\n",
    "        combined_inputs = torch.cat((encoder_outputs, hidden_state_expanded), 2).view(-1, 2*self.hidden_size)\n",
    "\n",
    "        attn_hidden = self.attention_layer1(combined_inputs)\n",
    "        attn_hidden = self.attention_layer2(attn_hidden)\n",
    "        attn_hidden = self.attention_layer3(attn_hidden)\n",
    "        attn_hidden = self.attention_layer4(attn_hidden)\n",
    "        weights = self.compute_weight(attn_hidden)\n",
    "        weights = weights.view(batch_size, seq_len)\n",
    "        weights_normalized = F.softmax(weights, dim=1)\n",
    "        context_vector = torch.bmm(weights_normalized.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        \n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89f30cfd-2b9b-4fe1-aaba-1b8e54ef9bbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        \n",
    "        self.Embedding = nn.Linear(4096, 512)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.lstm = nn.LSTM(512, 512, batch_first=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size, seq_len, feat_n = input.size()    \n",
    "        input = input.view(-1, feat_n)\n",
    "        input = self.Embedding(input)\n",
    "        input = self.dropout(input)\n",
    "        input = input.view(batch_size, seq_len, 512)\n",
    "\n",
    "        output, t = self.lstm(input)\n",
    "        hidden_state, context = t[0], t[1]\n",
    "        return output, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3795131f-ff8a-4099-969e-48b97f60df19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, vocab_size, word_dim, dropout_percentage=0.33):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = 512\n",
    "        self.output_size = output_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_dim = word_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, 1024)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.lstm = nn.LSTM(hidden_size+word_dim, hidden_size, batch_first=True)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.to_final_output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, encoder_last_hidden_state, encoder_output, targets=None, mode='train', tr_steps=None):\n",
    "        _, batch_size, _ = encoder_last_hidden_state.size()\n",
    "        \n",
    "        decoder_current_hidden_state = None if encoder_last_hidden_state is None else encoder_last_hidden_state\n",
    "        decoder_cxt = torch.zeros(decoder_current_hidden_state.size()).cuda()\n",
    "\n",
    "        decoder_current_input_word = Variable(torch.ones(batch_size, 1)).long().cuda()\n",
    "        seq_logProb = []\n",
    "        seq_predictions = []\n",
    "\n",
    "        targets = self.embedding(targets)\n",
    "        _, seq_len, _ = targets.size()\n",
    "\n",
    "        for i in range(seq_len-1):\n",
    "            threshold = self.teacher_forcing_ratio(training_steps=tr_steps)\n",
    "            if random.uniform(0.05, 0.995) > threshold: # returns a random float value between 0.05 and 0.995\n",
    "                current_input_word = targets[:, i]  \n",
    "            else: \n",
    "                current_input_word = self.embedding(decoder_current_input_word).squeeze(1)\n",
    "\n",
    "            context = self.attention(decoder_current_hidden_state, encoder_output)\n",
    "            lstm_input = torch.cat([current_input_word, context], dim=1).unsqueeze(1)\n",
    "            lstm_output, t = self.lstm(lstm_input, (decoder_current_hidden_state,decoder_cxt))\n",
    "            decoder_current_hidden_state=t[0]\n",
    "            logprob = self.to_final_output(lstm_output.squeeze(1))\n",
    "            seq_logProb.append(logprob.unsqueeze(1))\n",
    "            decoder_current_input_word = logprob.unsqueeze(1).max(2)[1]\n",
    "\n",
    "        seq_logProb = torch.cat(seq_logProb, dim=1)\n",
    "        seq_predictions = seq_logProb.max(2)[1]\n",
    "        return seq_logProb, seq_predictions\n",
    "        \n",
    "    def infer(self, encoder_last_hidden_state, encoder_output):\n",
    "        _, batch_size, _ = encoder_last_hidden_state.size()\n",
    "        decoder_current_hidden_state = None if encoder_last_hidden_state is None else encoder_last_hidden_state\n",
    "        decoder_current_input_word = Variable(torch.ones(batch_size, 1)).long()\n",
    "        decoder_c= torch.zeros(decoder_current_hidden_state.size())\n",
    "        seq_logProb = []\n",
    "        seq_predictions = []\n",
    "        assumption_seq_len = 28\n",
    "        \n",
    "        for i in range(assumption_seq_len-1):\n",
    "            current_input_word = self.embedding(decoder_current_input_word).squeeze(1)\n",
    "            context = self.attention(decoder_current_hidden_state, encoder_output)\n",
    "            lstm_input = torch.cat([current_input_word, context], dim=1).unsqueeze(1)\n",
    "            lstm_output,  t = self.lstm(lstm_input, (decoder_current_hidden_state,decoder_c))\n",
    "            decoder_current_hidden_state=t[0]\n",
    "            logprob = self.to_final_output(lstm_output.squeeze(1))\n",
    "            seq_logProb.append(logprob.unsqueeze(1))\n",
    "            decoder_current_input_word = logprob.unsqueeze(1).max(2)[1]\n",
    "\n",
    "        seq_logProb = torch.cat(seq_logProb, dim=1)\n",
    "        seq_predictions = seq_logProb.max(2)[1]\n",
    "        return seq_logProb, seq_predictions\n",
    "\n",
    "    def teacher_forcing_ratio(self, training_steps):\n",
    "        return (expit(training_steps/20 +0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb2d12cf-3810-4b0c-bc1b-fc671fdb9ee0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MODELS(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(MODELS, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self, avi_feat, mode, target_sentences=None, tr_steps=None):\n",
    "        encoder_outputs, encoder_last_hidden_state = self.encoder(avi_feat)\n",
    "        if mode == 'train':\n",
    "            seq_logProb, seq_predictions = self.decoder(encoder_last_hidden_state = encoder_last_hidden_state, encoder_output = encoder_outputs,\n",
    "                targets = target_sentences, mode = mode, tr_steps=tr_steps)\n",
    "        elif mode == 'inference':\n",
    "            seq_logProb, seq_predictions = self.decoder.infer(encoder_last_hidden_state=encoder_last_hidden_state, encoder_output=encoder_outputs)\n",
    "        return seq_logProb, seq_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47cd3e69-2cd4-488e-9456-5dba7a2e2fb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_loss(loss_fn, predictions, targets, lengths):\n",
    "    batch_size = len(predictions)\n",
    "    concatenated_predictions = None\n",
    "    concatenated_targets = None\n",
    "    is_first_batch = True \n",
    "\n",
    "    for i in range(batch_size):\n",
    "        current_prediction = predictions[i]\n",
    "        current_target = targets[i]\n",
    "        current_length = lengths[i] - 1 \n",
    "\n",
    "        # Trim the sequences based on the actual length\n",
    "        trimmed_prediction = current_prediction[:current_length]\n",
    "        trimmed_target = current_target[:current_length]\n",
    "\n",
    "        # Initialize or concatenate the sequences\n",
    "        if is_first_batch:\n",
    "            concatenated_predictions = trimmed_prediction\n",
    "            concatenated_targets = trimmed_target\n",
    "            is_first_batch = False \n",
    "        else:\n",
    "            concatenated_predictions = torch.cat((concatenated_predictions, trimmed_prediction), dim=0)\n",
    "            concatenated_targets = torch.cat((concatenated_targets, trimmed_target), dim=0)\n",
    "\n",
    "    # Compute the loss on the concatenated sequences\n",
    "    total_loss = loss_fn(concatenated_predictions, concatenated_targets)\n",
    "    avg_loss = total_loss / batch_size  #average loss\n",
    "\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a176ea9f-771e-4a02-8c0b-f96f5509ee4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def minibatch(data):\n",
    "    # Sort the input data by the length of the captions in descending order\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    video_data, captions = zip(*data)\n",
    "    video_tensor = torch.stack(video_data, 0)\n",
    "    \n",
    "    caption_lengths = [len(cap) for cap in captions]\n",
    "    \n",
    "    # Initialize a zero tensor for all captions based on the longest caption\n",
    "    target_tensor = torch.zeros(len(captions), max(caption_lengths)).long()\n",
    "    \n",
    "    # Fill in the target tensor with actual captions, padding the rest\n",
    "    for index, caption in enumerate(captions):\n",
    "        end = caption_lengths[index]\n",
    "        target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
    "    \n",
    "    return video_tensor, target_tensor, caption_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49280678-b235-4293-b0c3-b1a9467231be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(model, epoch, loss_fn, parameters, optimizer, train_loader):\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        avi_feats, ground_truths, lengths = batch\n",
    "        avi_feats, ground_truths = Variable(avi_feats).cuda(), Variable(ground_truths).cuda()\n",
    "        # avi_feats, ground_truths = Variable(avi_feats), Variable(ground_truths)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        seq_logProb, seq_predictions = model(avi_feats, target_sentences = ground_truths, mode = 'train', tr_steps = epoch)\n",
    "        ground_truths = ground_truths[:, 1:]  \n",
    "        loss = calculate_loss(loss_fn, seq_logProb, ground_truths, lengths)\n",
    "        print('Batch - ', batch_idx, ' Loss - ', loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"---------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    loss = loss.item()\n",
    "    return loss\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "94ba5a6b-0fdf-4fa2-a262-d0dc196b152e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(test_loader, model, i2w):\n",
    "    model.eval()\n",
    "    _ss = []\n",
    "    \n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "     \n",
    "        id, avi_feats = batch\n",
    "        avi_feats = avi_feats.cuda()\n",
    "        id, avi_feats = id, Variable(avi_feats).float()\n",
    "        \n",
    "        seq_logProb, seq_predictions = model(avi_feats, mode='inference')\n",
    "        test_predictions = seq_predictions\n",
    "        \n",
    "        result = [[i2w[x.item()] if i2w[x.item()] != '<UNK>' else 'something' for x in s] for s in test_predictions]\n",
    "        result = [' '.join(s).split('<EOS>')[0] for s in result]\n",
    "        rr = zip(id, result)\n",
    "        for r in rr:\n",
    "            _ss.append(r)\n",
    "    return _ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "edcf6efa-42a6-4670-93c6-9b139cf7ea49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file no:-  1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 131062 into shape (80,4096)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted Training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 30\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[43], line 7\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m label_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/training_data/feat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m files_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/training_label.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m TrainingData(label_file, files_dir, word_dict, w2i)\n\u001b[1;32m      8\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset \u001b[38;5;241m=\u001b[39m train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, collate_fn\u001b[38;5;241m=\u001b[39mminibatch)\n\u001b[1;32m     10\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m17\u001b[39m\n",
      "Cell \u001b[0;32mIn[29], line 6\u001b[0m, in \u001b[0;36mTrainingData.__init__\u001b[0;34m(self, label_file, files_dir, word_dict, w2i)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles_dir \u001b[38;5;241m=\u001b[39m files_dir\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_dict \u001b[38;5;241m=\u001b[39m word_dict\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavi \u001b[38;5;241m=\u001b[39m avi(label_file)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw2i \u001b[38;5;241m=\u001b[39m w2i\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_pair \u001b[38;5;241m=\u001b[39m annotate(files_dir, word_dict, w2i)\n",
      "Cell \u001b[0;32mIn[41], line 7\u001b[0m, in \u001b[0;36mavi\u001b[0;34m(label_file)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading file no:-  \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i))\n\u001b[1;32m      6\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 7\u001b[0m value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(label_file, file))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m80\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4096\u001b[39m:  \u001b[38;5;66;03m# If the array is smaller, pad it\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     padded_value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpad(value, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m80\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4096\u001b[39m \u001b[38;5;241m-\u001b[39m value\u001b[38;5;241m.\u001b[39msize), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/numpy/lib/npyio.py:432\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[1;32m    430\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mread_array(fid, allow_pickle\u001b[38;5;241m=\u001b[39mallow_pickle,\n\u001b[1;32m    433\u001b[0m                                  pickle_kwargs\u001b[38;5;241m=\u001b[39mpickle_kwargs,\n\u001b[1;32m    434\u001b[0m                                  max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/numpy/lib/format.py:831\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    829\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[1;32m    830\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 831\u001b[0m         array\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m=\u001b[39m shape\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 131062 into shape (80,4096)"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    i2w, w2i, word_dict = __preprocess()\n",
    "    with open('i2w.pickle', 'wb') as handle:\n",
    "        pickle.dump(i2w, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    label_file = 'data/training_data/feat'\n",
    "    files_dir = 'data/training_label.json'\n",
    "    train_dataset = TrainingData(label_file, files_dir, word_dict, w2i)\n",
    "    train_dataloader = DataLoader(dataset = train_dataset, batch_size=64, shuffle=True, num_workers=8, collate_fn=minibatch)\n",
    "    \n",
    "    epochs = 17\n",
    "    __dropout = 0.33\n",
    "\n",
    "    __encoder = EncoderLSTM()\n",
    "    __decoder = DecoderLSTM(512, len(i2w) +4, len(i2w) +4, 1024, __dropout)\n",
    "    model = MODELS(encoder=__encoder, decoder=__decoder)\n",
    "    \n",
    "    model = model.cuda()\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    params = model.parameters()\n",
    "    optimizer = optim.Adam(params, lr=0.0001)\n",
    "    loss_arr = []\n",
    "    for epoch in range(epochs):\n",
    "        loss = train(model, epoch+1, loss_function, params, optimizer, train_dataloader) \n",
    "        loss_arr.append(loss)\n",
    "\n",
    "    torch.save(model, \"{}/{}.h5\".format('SavedModel', 'model'))\n",
    "    print(\"Completed Training\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78af96d-cc02-42a5-afc6-5a31b4df6704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4b6a47-2272-474a-a5b6-563e2dfa23b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
